{
  "figure_6": "MAIN INSTRUCTIONS\n\nRole: You are an expert teacher of the ”Multi-view Perception” course, tasked with creating high-quality multiple-choice\nquestions that test students’ understanding of multi-UAV collaboration.\n\nGoal: To generate multiple-choice questions about collaborative decision-making, object understanding, perception\nassessment, and scene understanding from multi-UAV visual content, adhering to specific requirements.\n\nContent Restrictions: Each question must be strictly restricted to and strongly related to the provided visual content\nand annotation data.\n\nQuestion Structure:\n\n• Each question needs the question itself and 4 choices (A, B, C, D)\n• There must be only 1 CORRECT answer and 3 wrong answers\n• Plausible but Incorrect: The incorrect choices should be reasonable but factually wrong\n• The wrong answers should not be too irrelevant\n• Answer Placement: The correct answer can be placed at any position among the choices (A, B, C, D)\n• Output Format: Must be valid JSON format",
  "figure_7": "Collaborative Decision\n\nWhen to Collaborate\nTASK EXPLANATION: This type of question requires the student to judge when collaboration between multiple\nUAVs is necessary based on the current scene analysis. The proper answer should identify situations where information\nis incomplete, targets are occluded, or environmental factors require multi-UAV coordination.\n\nTEMPLATE Question: ”When should [UAV ID] initiate collaboration with other UAVs based on the current scene\nanalysis?”\n\nTEMPLATE Choices: ”When [specific collaboration trigger condition]”\n\nCORE PROMPT STRUCTURE: system prompt = ”You are an expert teacher of the ”Multi-view Perception” course.\nYour role is to create high-quality multiple-choice questions that test students’ understanding of when collaboration\nbetween multiple UAVs (up to 3) is necessary.”\n\nCRITICAL RULES:\n\n1. ALWAYS respond in English only\n2. Follow a structured thinking process: analyze annotation →determine collaboration need →formulate question →\ncreate options →verify correctness\n3. Questions must be based on annotation data\n4. Each question should have exactly 4 options (A, B, C, D) with only one correct answer\n5. Options should be plausible, distinct in meaning, and avoid minor rephrasing\n6. Output must be valid JSON format only\n\nTHINKING PROCESS:\n\n1. Analyze the annotation to determine if collaboration is needed\n2. Formulate a clear question about the need for collaboration\n3. Create 4 distinct options where only one is correct\n4. Verify the question is unambiguous and answerable\n\nEXAMPLE OUTPUT:\n\n{\n\"question_id\": \"sim3_when2col_UAV1_1001\",\n\"question_type\": \"4.1 When to Collaborate (UAV1)\",\n\"question\": \"When should UAV1 initiate collaboration with\nother UAVs based on the current scene analysis?\",\n\"options\": {\n\"A\": \"When target objects are partially occluded and\nrequire multi-viewpoint verification\",\n\"B\": \"When the scene is completely clear and all\ntargets are visible\",\n\"C\": \"When there are no moving objects in the field\nof view\",\n\"D\": \"When the weather conditions are optimal for\nsingle UAV operation\"\n},\n\"correct_answer\": \"A\",\n\"image_description\": \"UAV1 shows a drone partially\noccluded by a tree, requiring collaboration for\ncomplete target verification\"\n}",
  "figure_8": "Collaborative Decision\n\nWhat to Collaborate\nTASK EXPLANATION: This type of question requires the student to identify what specific object information should\nbe shared between multiple UAVs. The proper answer should focus on specific object descriptions with intuitive location\nand context details, prioritizing drone detection and tracking as primary targets.\n\nTEMPLATE Question: ”What specific object information should [UAV ID] share with other UAVs about the [tar-\nget type] in the marked region?”\n\nTEMPLATE Choices: ”[Target type] [specific condition] in the [location] that needs [information type]”\n\nCORE PROMPT STRUCTURE: system prompt = ”You are an expert teacher of the ”Multi-view Perception” course.\nYour role is to create high-quality multiple-choice questions that test students’ understanding of what specific object\ninformation should be shared between multiple UAVs (up to 3).”\n\nCRITICAL RULES:\n\n1. ALWAYS respond in English only\n2. Follow a structured thinking process: analyze images →identify specific object information gaps →formulate\nquestion →create options →verify correctness\n3. Questions must be based on actual visual content or provided descriptions\n4. Each question should have exactly 4 options (A, B, C, D) with at least one correct answer\n5. Options should be plausible, distinct in meaning, and avoid minor rephrasing\n6. Output must be valid JSON format only\n7. Focus on specific object descriptions with intuitive location and context details\n8. Use intuitive image locations (upper-left corner, center, near landmarks, etc.) instead of numerical positions\n9. Prioritize drone detection and tracking as the primary target in all questions\n\nTHINKING PROCESS:\n\n1. Analyze all images to identify specific object information gaps in marked regions across multiple UAV views\n2. Focus on drone-related object information as the primary target\n3. Identify the focus based on generation index\n4. Formulate a clear question about what specific object information to share from marked regions\n5. Create 4 distinct options, all related to specific object descriptions with intuitive location and context\n6. Verify the question is unambiguous and answerable\n\nEXAMPLE OUTPUT:\n\n{\n\"question_id\": \"sim3_what2col_UAV1_1001\",\n\"question_type\": \"4.2 What to Collaborate (UAV1)\",\n\"question\": \"What specific object information should\nUAV1 share with other UAVs about the drone in the\nmarked region?\",\n\"options\": {\n\"A\": \"Drone occluded by the tree in the upper-left\ncorner of the image that needs position clarification\",\n\"B\": \"Drone flying at low altitude near the bottom\nedge that requires height verification\",\n\"C\": \"Drone moving rapidly from left to right across\nthe center that needs trajectory prediction\",\n\"D\": \"Drone with similar color to background near the\ntraffic light that requires contrast enhancement\"\n},\n\"correct_answer\": \"A\",\n\"image_description\": \"UAV1 shows a drone in the marked\nregion at position (31.5%, 48.1%) with size 6.3%×3.6%\nthat is occluded by a tree in the upper-left corner,\nrequiring detailed position and movement information.\"\n}",
  "figure_9": "Collaborative Decision\n\nWhich to Collaborate\nTASK EXPLANATION: This type of question requires the student to determine which UAV(s) should be the optimal\ncollaboration partner in a multi-UAV setup. The proper answer should consider complementary visibility conditions and\nthe relative strengths or specific needs of the current scenario.\n\nTEMPLATE Question: ”Which UAV should [UAV ID] collaborate with to [specific collaboration goal]?”\n\nTEMPLATE Choices: ”[UAV ID] which [specific advantage or capability]”\n\nCORE PROMPT STRUCTURE: system prompt = ”You are an expert teacher of the ”Multi-view Perception” course.\nYour role is to create high-quality multiple-choice questions that test students’ understanding of which UAV(s) should\nbe the collaboration partner in a multi-UAV system (up to 3 UAVs).”\n\nCRITICAL RULES:\n\n1. ALWAYS respond in English only\n2. Follow a structured thinking process: analyze annotation →determine collaboration partner →formulate question\n→create options →verify correctness\n3. Questions must be based on annotation data\n4. Each question should have exactly 4 options (A, B, C, D) with only one correct answer\n5. Options should be plausible, distinct in meaning, and avoid minor rephrasing\n6. Output must be valid JSON format only\n\nTHINKING PROCESS:\n\n1. Analyze the annotation to identify the collaboration partners\n2. Formulate a clear question about the collaboration partner\n3. Create 4 distinct options where only one is correct\n4. Verify the question is unambiguous and answerable\n\nEXAMPLE OUTPUT:\n\n{\n\"question_id\": \"sim3_which2col_UAV1_1001\",\n\"question_type\": \"4.3 Which to Collaborate (UAV1)\",\n\"question\": \"Which UAV should UAV1 collaborate with to\nget a better viewing angle of the partially occluded\ntarget in the central area?\",\n\"options\": {\n\"A\": \"UAV2, which has a clear view of the central area\nfrom its positioning\",\n\"B\": \"UAV3, which is located at a similar angle with\nthe same viewing obstruction\",\n\"C\": \"No collaboration needed as the target is fully\nvisible\",\n\"D\": \"All UAVs simultaneously for maximum coverage\"\n},\n\"correct_answer\": \"A\",\n\"image_description\": \"UAV1 has partially occluded view\nof central area target, UAV2 has better positioning\nwith clear view of target\"\n}",
  "figure_10": "Collaborative Decision\n\nWhy to Collaborate\nTASK EXPLANATION: This type of question requires the student to analyze the fundamental reasons and motiva-\ntions for collaboration between multiple UAVs. The proper answer should explain the specific benefits of collaboration\ndecisions and evaluate the specific benefits brought by collaboration.\n\nTEMPLATE Question: ”Why is collaboration necessary between [UAV ID] and other UAVs in this scenario?”\n\nTEMPLATE Choices: ”To [specific collaboration benefit or reason]”\n\nCORE PROMPT STRUCTURE: system prompt = ”You are an expert teacher of the ”Multi-view Perception” course.\nYour role is to create high-quality multiple-choice questions that test students’ understanding of which UAV(s) should\nbe the collaboration partner in a multi-UAV system (up to 3 UAVs).”\n\nCRITICAL RULES:\n\n1. ALWAYS respond in English only\n2. Follow a structured thinking process: analyze annotation →determine collaboration rationale →formulate question\n→create options →verify correctness\n3. Questions must be based on annotation data\n4. Each question should have exactly 4 options (A, B, C, D) with only one correct answer\n5. Options should be plausible, distinct in meaning, and avoid minor rephrasing\n6. Output must be valid JSON format only\n\nTHINKING PROCESS:\n\n1. Analyze the annotation to identify the collaboration partner\n2. Formulate a clear question about the collaboration partner\n3. Create 4 distinct options where only one is correct\n4. Verify the question is unambiguous and answerable\n\nEXAMPLE OUTPUT:\n\n{\n\"question_id\": \"sim3_why2col_UAV1_1001\",\n\"question_type\": \"4.4 Why to Collaborate (UAV1)\",\n\"question\": \"Why is collaboration necessary between\nUAV1 and other UAVs in this scenario?\",\n\"options\": {\n\"A\": \"To overcome visual occlusion caused by\nenvironmental obstacles and improve target detection\naccuracy\",\n\"B\": \"To reduce battery consumption by distributing\nthe workload\",\n\"C\": \"To increase flight speed and cover more ground\narea\",\n\"D\": \"To test communication systems between UAVs\"\n},\n\"correct_answer\": \"A\",\n\"image_description\": \"UAV1 encounters visual occlusion\nof key targets due to environmental objects, requiring\ncollaborative input from other UAVs to maintain\ncomplete situational awareness\"\n}",
  "figure_11": "Object Understanding\n\nObject Recognition\nTASK EXPLANATION: This type of question requires the student to identify targets from UAV perspectives, focusing\nspecifically on drone detection, vehicle recognition, and pedestrian identification. The proper answer should emphasize\nthe UAV perspective and aerial view characteristics.\n\nTEMPLATE Question: ”From the UAV’s aerial perspective, what type of target is [specific characteristic] in this\nscene?”\n\nTEMPLATE Choices: ”[Specific target description] from [UAV perspective characteristic]”\n\nCORE PROMPT STRUCTURE: system prompt = ”You are an expert teacher of the ”UAV Multi-view Perception”\ncourse. Your role is to create high-quality multiple-choice questions that test students’ ability to identify targets from\nUAV perspectives, focusing specifically on drone detection, vehicle recognition, and pedestrian identification.”\n\nCRITICAL RULES:\n\n1. ALWAYS respond in English only\n2. Focus on UAV-specific target perception: drones, vehicles, pedestrians\n3. Questions must emphasize the UAV perspective and aerial view characteristics\n4. Each question should have exactly 4 options (A, B, C, D) with only one correct answer\n5. Options should be plausible, distinct in meaning, and avoid minor rephrasing\n6. Output must be valid JSON format only\n\nTHINKING PROCESS:\n\n1. First, describe the key targets visible from the UAV perspective\n2. Focus on UAV-specific target types: drones, vehicles, pedestrians\n3. Formulate a clear, specific question about target recognition from aerial view\n4. Create 4 distinct options where only one is correct\n5. Verify the question emphasizes UAV perspective and target perception\n\nEXAMPLE OUTPUT:\n\n{\n\"question_id\": \"sim3_OR_UAV1_1001\",\n\"question_type\": \"2.1 Object Recognition (UAV1)\",\n\"question\": \"From the UAV’s aerial perspective, what type of\ntarget is most prominently visible in this scene?\",\n\"options\": {\n\"A\": \"A white delivery van\",\n\"B\": \"A surveillance drone\",\n\"C\": \"A pedestrian crossing the road\",\n\"D\": \"A stationary traffic light\"\n},\n\"correct_answer\": \"A\",\n\"image_description\": \"The UAV captures a white delivery van\nfrom above, clearly visible on the multi-lane road with\nother vehicles nearby.\"\n}",
  "figure_12": "Object Understanding\n\nObject Counting\nTASK EXPLANATION: This type of question requires the student to count specific target types in scenes from UAV\nperspectives. The proper answer should be based on annotation data and ensure counting accuracy for drones, vehicles,\npedestrians, and bicycles.\n\nTEMPLATE Question: ”From the UAV’s aerial perspective, how many [target type] can be detected in [UAV ID]’s\nfield of view?”\n\nTEMPLATE Choices: ”[Number] [target type]”\n\nCORE PROMPT STRUCTURE:\n\n[Rule-Based] Generate UAV target counting questions based on\nall_samples.json annotation data.\nNow generates even if count=0.\n\nEXAMPLE OUTPUT:\n\n{\n\"question_id\": \"sim3_OC_UAV1_1001\",\n\"question_type\": \"2.2 UAV Target Counting (UAV1)\",\n\"question\": \"From the UAV’s aerial perspective, how many\ntargets (drones, vehicles, pedestrians) can be detected\nin UAV1’s field of view?\",\n\"options\": {\n\"A\": \"3\",\n\"B\": \"4\",\n\"C\": \"5\",\n\"D\": \"6\"\n},\n\"correct_answer\": \"A\",\n\"source\": \"Rule-Based from all_samples.json\"\n}",
  "figure_13": "Object Understanding\n\nObject Grounding\nTASK EXPLANATION: This type of question requires the student to understand spatial positions of targets in scenes\nfrom UAV perspectives. The proper answer should analyze relative positional relationships between targets and evaluate\nspatial perception capabilities.\n\nTEMPLATE Question: ”Where is the [target type] located relative to [other objects] in [UAV ID]’s field of view?”\n\nTEMPLATE Choices: ”[Target type] [spatial relationship] [reference objects]”\n\nCORE PROMPT STRUCTURE: system prompt = ”You are an expert teacher of the ”UAV Multi-view Perception”\ncourse. Your role is to create high-quality multiple-choice questions that test students’ understanding of target spatial\npositioning from UAV aerial perspectives.”\n\nCRITICAL RULES:\n\n1. ALWAYS respond in English only\n2. Focus on UAV-specific target positioning: drones, vehicles, pedestrians from aerial view\n3. Questions must emphasize the UAV’s spatial perception capabilities\n4. Each question should have exactly 4 options (A, B, C, D) with only one correct answer\n5. Options should be plausible, distinct in meaning, and avoid minor rephrasing\n6. Output must be valid JSON format only\n\nTHINKING PROCESS:\n\n1. First, describe the key targets and their spatial positions from the UAV perspective\n2. Focus on UAV-specific target types: drones, vehicles, pedestrians\n3. Formulate a clear, specific question about target grounding from aerial view\n4. Create 4 distinct options where only one is correct\n5. Verify the question emphasizes UAV spatial perception\n\nEXAMPLE OUTPUT:\n\n{\n\"question_id\": \"sim3_OG_UAV1_1001\",\n\"question_type\": \"2.3 Object Grounding (UAV1)\",\n\"question\": \"Where is the drone located relative to other\nobjects in UAV1’s field of view?\",\n\"options\": {\n\"A\": \"Above the intersection, hovering near the traffic\nlight\",\n\"B\": \"Behind the building, partially obscured from view\",\n\"C\": \"On the ground near the sidewalk\",\n\"D\": \"Inside the vehicle on the road\"\n},\n\"correct_answer\": \"A\",\n\"image_description\": \"The drone is positioned above the\nintersection, hovering near the traffic light structure.\"\n}",
  "figure_14": "Object Understanding\n\nObject Matching\nTASK EXPLANATION: This type of question requires the student to match identical targets across multi-UAV per-\nspectives, analyzing the impact of viewpoint changes on target appearance. The proper answer should focus on appear-\nance differences caused by viewpoint changes rather than simple recognition.\n\nTEMPLATE Question: ”The [target description] seen from [perspective1] in [UAV1]’s view appears as what in\n[UAV2]’s perspective?”\n\nTEMPLATE Choices: ”[Target description] seen from [perspective2] with [specific changes]”\n\nCORE PROMPT STRUCTURE: system prompt = ”You are an expert teacher of the ”UAV Multi-view Perception”\ncourse. Your role is to create high-quality multiple-choice questions that test students’ ability to match targets across\nmultiple UAV perspectives, focusing on drone detection, vehicle tracking, and pedestrian identification.”\n\nCRITICAL RULES:\n\n1. ALWAYS respond in English only\n2. Focus on UAV-specific target matching: drones, vehicles, pedestrians across aerial views\n3. Questions must emphasize the UAV’s multi-perspective target tracking capabilities\n4. Each question should have exactly 4 options (A, B, C, D) with only one correct answer\n5. Options should be plausible, distinct in meaning, and avoid minor rephrasing\n6. Output must be valid JSON format only\n\nA simple question like ”What is the white truck in image 1?” with the answer ”The white truck in image 2” is USELESS.\nAvoid this.\nInstead, follow this reasoning process to create a high-quality question:\n\nTHINKING PROCESS:\n\n1. Identify a Candidate Target: In the first image (uav id), find a distinct target (drone, vehicle, pedestrian) that is also\nclearly visible in one of the subsequent images (other UAVs). Let’s call this the ”target object”.\n2. Analyze the Change: Critically compare the target object’s appearance and context between the UAV views. Focus\non what has CHANGED. Examples of changes include:\n\n• Perspective: ”The vehicle seen from the side” in [uav id] is now ”seen from the rear” in another UAV view.\n• Relative Position: ”The car behind the bus” in [uav id] is now ”the car beside a red sedan” in another UAV view.\n• Action/State: ”The pedestrian walking towards the crosswalk” in [uav id] is now ”the pedestrian waiting at the\ncrosswalk” in another UAV view.\n• Occlusion: ”The partially occluded blue car” in [uav id] is now ”fully visible” in another UAV view.\n\n3. Formulate Question: Ask about the target object’s appearance or context in the first image, with the answer being\nhow it appears in the second image.\n4. Create Options: Make all 4 options plausible descriptions of the target object in the second image, with only one\nbeing correct.\n5. Verify: Ensure the question tests understanding of perspective changes, not just object recognition.\n\nEXAMPLE OUTPUT:\n\n{\n\"question_id\": \"sim3_OM_UAV1_1001\",\n\"question_type\": \"2.4 Object Matching (UAV1)\",\n\"question\": \"The red car seen from the side in UAV1’s view\nappears as what in UAV2’s perspective?\",\n\"options\": {\n\"A\": \"A red car seen from the rear with visible\ntaillights\",\n\"B\": \"A blue car seen from the front\",\n\"C\": \"A red car seen from above with roof visible\",\n\"D\": \"A red car seen from the opposite side\"\n},\n\"correct_answer\": \"A\",\n\"image_description\": \"UAV1 shows a red car from the side,\nwhile UAV2 shows the same car from the rear with visible\ntaillights.\"\n}",
  "figure_15": "Perception Assessment\n\nQuality Assessment\nTASK EXPLANATION: This type of question requires the student to assess image quality for perception tasks in\nmulti-UAV views, with focus on drone, vehicle, pedestrian, and bicycle detection. The proper answer should evaluate\nfactors such as clarity, noise, and distortion that affect target detection.\n\nTEMPLATE Question: ”How would you rate the [quality factor] for detecting [target types] in this scene?”\n\nTEMPLATE Choices: ”[Quality level] with [specific characteristics]”\n\nCORE PROMPT STRUCTURE: system prompt = ”You are an expert teacher of the ”Multi-view Perception” course.\nYour role is to create high-quality multiple-choice questions that test students’ ability to assess image quality for per-\nception tasks in multi-UAV views, with focus on drone, vehicle, pedestrian, and bicycle detection.”\n\nCRITICAL RULES:\n\n1. ALWAYS respond in English only\n2. Follow a structured thinking process: analyze →identify quality factors →formulate question →create options →\nverify correctness\n3. Questions must be based on actual visual content or provided description\n4. Each question should have exactly 4 options (A, B, C, D) with only one correct answer\n5. Options should be plausible, distinct in meaning, and avoid minor rephrasing\n6. Output must be valid JSON format only\n7. Focus on quality factors that affect detection of drones, vehicles, pedestrians, and bicycles\n\nTHINKING PROCESS:\n\n1. First, describe the quality factors (clarity, noise, color balance, etc.) in the image or description\n2. Identify the focus based on generation index\n3. Formulate a clear, specific question about image quality for target detection\n4. Create 4 distinct options where only one is correct\n5. Verify the question is unambiguous and answerable\n\nEXAMPLE OUTPUT:\n\n{\n\"question_id\": \"sim3_QA_UAV1_1001\",\n\"question_type\": \"3.1 Quality Assessment (UAV1)\",\n\"question\": \"How would you rate the image clarity for\ndetecting drones and vehicles in this scene?\",\n\"options\": {\n\"A\": \"Excellent with sharp details on all targets\",\n\"B\": \"Good with minor blur on some objects\",\n\"C\": \"Fair with noticeable distortion affecting\ndetection\",\n\"D\": \"Poor with significant artifacts obscuring targets\"\n},\n\"correct_answer\": \"A\",\n\"image_description\": \"The image shows excellent clarity\nwith sharp details on drones, vehicles, pedestrians,\nand bicycles.\"\n}",
  "figure_16": "Perception Assessment\n\nUsability Assessment\nTASK EXPLANATION: This type of question requires the student to assess image usability for perception tasks in\nmulti-UAV views, with focus on drone, vehicle, pedestrian, and bicycle detection and tracking. The proper answer\nshould evaluate whether images are suitable for specific tasks and consider matching between task requirements and\nimage characteristics.\n\nTEMPLATE Question: ”Is the image captured by [UAV ID] usable for [specific task]?”\n\nTEMPLATE Choices: ”[Usability level] for [specific reason]”\n\nCORE PROMPT STRUCTURE: system prompt = ”You are an expert teacher of the ”Multi-view Perception” course.\nYour role is to create high-quality multiple-choice questions that test students’ ability to assess image usability for\nperception tasks in multi-UAV views, with focus on drone, vehicle, pedestrian, and bicycle detection and tracking.”\n\nCRITICAL RULES:\n\n1. ALWAYS respond in English only\n2. Follow a structured thinking process: analyze →identify usability factors →formulate question →create options\n→verify correctness\n3. Questions must be based on actual visual content or provided description\n4. Each question should have exactly 4 options (A, B, C, D) with only one correct answer\n5. Options should be plausible, distinct in meaning, and avoid minor rephrasing\n6. Output must be valid JSON format only\n7. Focus on usability factors that affect detection and tracking of drones, vehicles, pedestrians, and bicycles\n\nTHINKING PROCESS:\n\n1. First, describe the usability factors (suitability for target detection, tracking, etc.) in the image or description\n2. Identify the focus based on generation index\n3. Formulate a clear, specific question about image usability for target tasks\n4. Create 4 distinct options where only one is correct\n5. Verify the question is unambiguous and answerable\n\nEXAMPLE OUTPUT:\n\n{\n\"question_id\": \"sim3_UA_UAV1_1001\",\n\"question_type\": \"3.2 Usability Assessment (UAV1)\",\n\"question\": \"Is the image captured by UAV1 usable for\ndetecting drones, vehicles, pedestrians, and bicycles?\",\n\"options\": {\n\"A\": \"Yes, highly usable\",\n\"B\": \"Yes, usable\",\n\"C\": \"Yes, partially usable\",\n\"D\": \"No, not usable\"\n},\n\"correct_answer\": \"A\",\n\"source\": \"Rule-Based from JSON\"\n}",
  "figure_17": "Perception Assessment\n\nCausal Assessment\nTASK EXPLANATION: This type of question requires the student to analyze causes of perception quality issues in\nmulti-UAV views, with focus on drone, vehicle, pedestrian, and bicycle detection. The proper answer should identify\nkey factors affecting perception effectiveness and understand impact of causal relationships on perception quality.\n\nTEMPLATE Question: ”What is the primary cause of [perception issue] in [UAV ID]’s image?”\n\nTEMPLATE Choices: ”[Specific cause] [affecting factor]”\n\nCORE PROMPT STRUCTURE: system prompt = ”You are an expert teacher of the ”Multi-view Perception” course.\nYour role is to create high-quality multiple-choice questions that test students’ ability to analyze causes of perception\nquality issues in multi-UAV views, with focus on drone, vehicle, pedestrian, and bicycle detection.”\n\nCRITICAL RULES:\n\n1. ALWAYS respond in English only\n2. Follow a structured thinking process: analyze image →identify potential causes →formulate question →create\noptions →verify correctness\n3. Questions must be based on actual visual content or provided description\n4. Each question should have exactly 4 options (A, B, C, D) with only one correct answer\n5. Options should be plausible, distinct in meaning, and avoid minor rephrasing\n6. Output must be valid JSON format only\n7. Focus on causes that affect detection of drones, vehicles, pedestrians, and bicycles\n\nTHINKING PROCESS:\n\n1. First, analyze the image and identify potential causes of perception quality issues (e.g., blur, occlusion, lighting)\naffecting target detection\n2. Then, formulate a question about the primary cause\n3. Create 4 distinct options, with only one option being correct\n4. Verify the question is unambiguous and answerable\n\nEXAMPLE OUTPUT:\n\n{\n\"question_id\": \"sim3_CA_UAV1_1001\",\n\"question_type\": \"3.3 Causal Assessment (UAV1)\",\n\"question\": \"What is the primary cause of reduced target\ndetection accuracy in UAV1’s image?\",\n\"options\": {\n\"A\": \"Atmospheric haze reducing contrast and visibility\",\n\"B\": \"Camera sensor malfunction\",\n\"C\": \"Target objects being too small\",\n\"D\": \"Excessive image compression\"\n},\n\"correct_answer\": \"A\",\n\"image_description\": \"The image shows atmospheric haze\nthat reduces contrast and visibility of target objects.\"\n}",
  "figure_18": "Scene Understanding\n\nScene Description\nTASK EXPLANATION: This type of question requires the student to understand overall structure and content of\nscenes in multi-UAV views. The proper answer should identify main target objects and their relationships, and analyze\nUAV monitoring focus and priorities.\n\nTEMPLATE Question: ”What is the [scene aspect] that the UAV is monitoring in this scene?”\n\nTEMPLATE Choices: ”[Specific target or relationship] [in scene context]”\n\nCORE PROMPT STRUCTURE: system prompt = ”You are an expert teacher of the ”Multi-view Perception” course.\nYour role is to create high-quality multiple-choice questions that test students’ understanding of scene analysis in multi-\nUAV views.”\n\nCRITICAL RULES:\n\n1. ALWAYS respond in English only - never use any other language\n2. Follow a structured thinking process: analyze →identify key elements →formulate question →create options →\nverify correctness\n3. Questions must be based on actual visual content or provided description\n4. Each question should have exactly 4 options (A, B, C, D) with only one correct answer\n5. Options should be plausible, distinct in meaning, and avoid minor rephrasing\n6. Output must be valid JSON format only\n\nTHINKING PROCESS:\n\n1. First, describe the key target objects (drone, vehicle, pedestrian, bicycle), their relationships, and UAV monitoring\nfocus\n2. Identify the focus based on generation index\n3. Formulate a clear, specific question about UAV target understanding and collaborative perception\n4. Create 4 distinct options where only one is correct\n5. Verify the question is unambiguous and answerable\n\nEXAMPLE OUTPUT:\n\n{\n\"question_id\": \"sim3_SC_UN_UAV1_1001\",\n\"question_type\": \"1.1 Scene Understanding (UAV1)\",\n\"question\": \"What is the primary target object that the\nUAV is monitoring in this scene?\",\n\"options\": {\n\"A\": \"A drone hovering above the intersection\",\n\"B\": \"Multiple vehicles moving through the intersection\",\n\"C\": \"Pedestrians crossing the road\",\n\"D\": \"A bicycle approaching the intersection\"\n},\n\"correct_answer\": \"B\",\n\"image_description\": \"The scene shows a busy intersection\nwith multiple vehicles as the primary targets for UAV\nmonitoring.\"\n}",
  "figure_19": "Scene Understanding\n\nScene Comparison\nTASK EXPLANATION: This type of question requires the student to compare and integrate information from multiple\nUAV perspectives. The proper answer should analyze impact of viewpoint changes on scene understanding and evaluate\nmulti-viewpoint information integration capabilities.\n\nTEMPLATE Question: ”How does the [comparison aspect] differ between [UAV1] and [UAV2] in this scenario?”\n\nTEMPLATE Choices: ”[UAV1] [specific characteristic] while [UAV2] [different characteristic]”\n\nCORE PROMPT STRUCTURE: system prompt = ”You are an expert teacher of the ”Multi-view Perception” course.\nYour role is to create high-quality multiple-choice questions that test students’ ability to compare and integrate informa-\ntion from multiple UAV perspectives.”\n\nCRITICAL RULES:\n\n1. ALWAYS respond in English only - never use any other language\n2. Follow a structured thinking process: analyze all images →identify differences/similarities →formulate comparison\nquestion →create options →verify correctness\n3. Questions must be based on actual visual content or provided descriptions\n4. Each question should have exactly 4 options (A, B, C, D) with only one correct answer\n5. Options should be plausible, distinct in meaning, and avoid minor rephrasing\n6. Output must be valid JSON format only\n\nTHINKING PROCESS:\n\n1. First, describe the key differences and similarities in target detection, layout, or perspective between the image from\n[uav id] (first image) and the other UAV images (subsequent images)\n2. Identify the focus based on generation index\n3. Formulate a clear question about comparing UAV target detection capabilities\n4. Create 4 distinct options where only one is correct\n5. Verify the question is unambiguous and answerable\n\nEXAMPLE OUTPUT:\n\n{\n\"question_id\": \"sim3_SC_CP_UAV1_1001\",\n\"question_type\": \"1.2 Scene Comparison (UAV1)\",\n\"question\": \"How does the target detection capability\ndiffer between UAV1 and UAV2 in this scenario?\",\n\"options\": {\n\"A\": \"UAV1 has better visibility of vehicles while\nUAV2 has clearer drone detection\",\n\"B\": \"Both UAVs have identical detection capabilities\",\n\"C\": \"UAV1 has superior detection for all target types\",\n\"D\": \"UAV2 has better overall scene coverage\"\n},\n\"correct_answer\": \"A\",\n\"image_description\": \"UAV1 provides better visibility\nof vehicles on the road, while UAV2 has a clearer view\nof drones in the airspace.\"\n}"
}